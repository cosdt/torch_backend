#include <chrono>
#include <sstream>
#include <thread>
#include <unordered_map>

#include <torch/csrc/Exceptions.h>
#include <torch/csrc/Generator.h>
#include <torch/csrc/THP.h>
#include <torch/csrc/autograd/generated/VariableType.h>
#include <torch/csrc/profiler/python/combined_traceback.h>
#include <torch/csrc/utils/device_lazy_init.h>
#include <torch/csrc/utils/pybind.h>
#include <torch/csrc/utils/python_arg_parser.h>
#include <torch/csrc/utils/python_numbers.h>
#include <torch/csrc/utils/python_strings.h>

#include "csrc/aten/generated/python_functions.h"
#include "csrc/npu/NPUFunctions.h"
#include "csrc/npu/NPUGeneratorImpl.h"
#include "csrc/npu/NPUStream.h"
#include "npu/acl/include/acl/acl.h"
#include "npu/aten/common/SetNpu.h"
#include "npu/core/NPUException.h"
#include "npu/core/NpuVariables.h"
#include "npu/core/OverflowUtils.h"
#include "npu/core/register/OptionRegister.h"
#include "torch_npu/csrc/npu/Module.h"

static PyObject* THNPModule_initExtension(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS {
    pybind11::gil_scoped_release no_gil;
    at::globalContext().lazyInitPrivateUse1();
  }
  auto m = THPObjectPtr(PyImport_ImportModule("torch.npu"));
  if (!m) {
    throw python_error();
  }

  auto set_module_attr = [&](const char* name, PyObject* v) {
    // PyObject_SetAttrString doesn't steal reference. So no need to incref.
    if (PyObject_SetAttrString(m, name, v) < 0) {
      throw python_error();
    }
  };
  c10::DeviceIndex num_npus = c10_npu::device_count();
  auto default_npu_generators = PyTuple_New(static_cast<Py_ssize_t>(num_npus));
  for (c10::DeviceIndex i = 0; i < num_npus; i++) {
    auto gen = at_npu::detail::getDefaultNPUGenerator(i);
    auto cast_gen = (THPGenerator*)THPGenerator_initDefaultGenerator(gen);
    // This reference is meant to be given away, so no need to incref here.
    PyTuple_SetItem(default_npu_generators, i, (PyObject*)cast_gen);
  }
  at_npu::autograd::generated::initialize_autogenerated_functions(m);
  set_module_attr("default_generators", default_npu_generators);

  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_getCurrentStream_wrap(
    PyObject* /* unused */,
    PyObject* device_index) {
  HANDLE_TH_ERRORS
  TORCH_CHECK(
      THPUtils_checkLong(device_index),
      "invalid argument to getCurrentStream",
      PTA_ERROR(ErrCode::PARAM));
  c10::DeviceIndex device = THPUtils_unpackDeviceIndex(device_index);
  auto stream = c10_npu::getCurrentNPUStream(device);
  PyObject* output_tuple = PyTuple_New(3);
  PyTuple_SetItem(
      output_tuple, 0, THPUtils_packInt64(static_cast<int64_t>(stream.id())));
  PyTuple_SetItem(
      output_tuple,
      1,
      THPUtils_packInt64(static_cast<int64_t>(stream.device_index())));
  PyTuple_SetItem(
      output_tuple,
      2,
      THPUtils_packInt64(static_cast<int64_t>(stream.device_type())));
  return output_tuple;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_getDefaultStream_wrap(
    PyObject* self /* unused */,
    PyObject* device_index) {
  HANDLE_TH_ERRORS
  TORCH_CHECK(
      THPUtils_checkLong(device_index),
      "invalid argument to getDefaultStream",
      PTA_ERROR(ErrCode::PARAM));
  c10::DeviceIndex device = THPUtils_unpackDeviceIndex(device_index);
  auto stream = c10_npu::getDefaultNPUStream(device);
  PyObject* output_tuple = PyTuple_New(3);
  PyTuple_SetItem(
      output_tuple, 0, THPUtils_packInt64(static_cast<int64_t>(stream.id())));
  PyTuple_SetItem(
      output_tuple,
      1,
      THPUtils_packInt64(static_cast<int64_t>(stream.device_index())));
  PyTuple_SetItem(
      output_tuple,
      2,
      THPUtils_packInt64(static_cast<int64_t>(stream.device_type())));
  return output_tuple;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_setStream_wrap(
    PyObject* self,
    PyObject* args,
    PyObject* kwargs) {
  HANDLE_TH_ERRORS
  int64_t stream_id = 0;
  int64_t device_index = 0;
  int64_t device_type = 0;

  // NOLINTNEXTLINE(modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)
  constexpr const char* kwlist[] = {
      "stream_id", "device_index", "device_type", nullptr};
  if (!PyArg_ParseTupleAndKeywords(
          args,
          kwargs,
          "|LLL",
          const_cast<char**>(kwlist),
          &stream_id,
          &device_index,
          &device_type)) {
  }

  auto stream = c10_npu::NPUStream::unpack3(
      stream_id,
      static_cast<c10::DeviceIndex>(device_index),
      static_cast<c10::DeviceType>(device_type));

  auto device = c10_npu::current_device();
  if (device != stream.device_index()) {
    c10_npu::set_device(stream.device_index());
  }
  c10_npu::setCurrentNPUStream(stream);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_is_jit_compile_false_wrap(
    PyObject* self,
    PyObject* noargs) {
  HANDLE_TH_ERRORS
  pybind11::gil_scoped_release no_gil;
  static const std::string jit_compile_option_name = "jitCompile";
  auto option_value = c10_npu::option::GetOption(jit_compile_option_name);
  if (option_value.has_value() && (option_value.value() == "disable")) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

// We need to ensure that as long as a thread will NEVER loose the GIL as long
// as it holds the NPU mutex. Otherwise another thread might be scheduled and
// try to e.g. allocate a new tensor which will cause a deadlock. It's enough to
// have a single global, because it can be only set once (npuMutex is not
// recursive) by the thread that owns the mutex (obviously there can be only one
// such thread).
static PyGILState_STATE npuMutexGILState;

PyObject* THNPModule_npuLockMutex(PyObject* module, PyObject* noargs) {
  auto mutex = c10_npu::getFreeMutex();
  // This has to be a busy loop because we **absolutely need to** hold the GIL
  // or it's a recipe for a deadlock otherwise (if we let other Python threads
  // run while we have the cudaMutex, but not the GIL, they might try to e.g.
  // free a CUDA tensor and acquire the cudaMutex without giving up the GIL,
  // because it happens deep within THC).
  while (true) {
    if (mutex->try_lock()) {
      break;
    }
    {
      pybind11::gil_scoped_release no_gil;
      std::this_thread::sleep_for(std::chrono::microseconds(10));
    }
  }

  npuMutexGILState = PyGILState_Ensure();
  Py_RETURN_NONE;
}

PyObject* THNPModule_npuUnlockMutex(PyObject* module, PyObject* noargs) {
  auto mutex = c10_npu::getFreeMutex();
  PyGILState_Release(npuMutexGILState);
  mutex->unlock();
  Py_RETURN_NONE;
}

PyObject* THNPModule_setOption_wrap(PyObject* self, PyObject* arg) {
  HANDLE_TH_ERRORS

  if (!PyDict_Check(arg)) {
    throw torch::TypeError(
        "npu option must be a dict." + PTA_ERROR(ErrCode::TYPE));
  }

  PyObject* key = nullptr;
  PyObject* value = nullptr;
  Py_ssize_t pos = 0;
  std::map<std::string, std::string> option;

  while (PyDict_Next(arg, &pos, &key, &value)) {
    if (key == nullptr || !PyUnicode_Check(key)) {
      throw torch::TypeError(
          "option name is nullptr or is not string." +
          PTA_ERROR(ErrCode::TYPE));
    }

    if (value == nullptr || !PyUnicode_Check(value)) {
      throw torch::TypeError(
          "option value is nullptr or is not string." +
          PTA_ERROR(ErrCode::TYPE));
    }

    const char* pKey = PyUnicode_AsUTF8(key);
    const char* pValue = PyUnicode_AsUTF8(value);
    option[pKey] = pValue;
  }
  torch::utils::device_lazy_init(at::kPrivateUse1);
  {
    pybind11::gil_scoped_release no_gil;
    c10_npu::option::SetOption(option);
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_set_run_yet_variable_to_false_wrap(
    PyObject* self,
    PyObject* noargs) {
  HANDLE_TH_ERRORS
  torch::utils::set_requires_device_init(at::DeviceType::PrivateUse1, true);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_npu_get_soc_version(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  return PyLong_FromLong(static_cast<long>(c10_npu::GetSocVersion()));
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_npu_is_support_inf_nan(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  if (c10_npu::IsSupportInfNan()) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_npu_is_bf16_supported(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  if (c10_npu::IsBF16Supported()) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_enable_overflow_npu(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  torch_npu::utils::OverflowUtil::GetInstance()->EnableOverflowNpu();
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_check_overflow_npu(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  auto has_overflow =
      torch_npu::utils::OverflowUtil::GetInstance()->CheckOverflowNpu();
  if (has_overflow) {
    Py_RETURN_TRUE;
  } else {
    Py_RETURN_FALSE;
  }
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_clear_overflow_npu(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  torch_npu::utils::OverflowUtil::GetInstance()->ClearOverflowNpu();
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_getOption_wrap(PyObject* self, PyObject* option_type) {
  HANDLE_TH_ERRORS
  TORCH_CHECK(
      THPUtils_checkString(option_type),
      "invalid argument to option_type,option_type must string!",
      PTA_ERROR(ErrCode::PARAM));
  std::string option_type_str = THPUtils_unpackString(option_type);
  auto option_key = c10_npu::option::GetOption(option_type_str);
  if (option_key.has_value()) {
    return PyBytes_FromString(option_key.value().c_str());
  }
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_npu_set_sync_debug_mode(PyObject* _unused, PyObject* arg) {
  HANDLE_TH_ERRORS
  TORCH_NPU_WARN_ONCE(
      "Synchronization debug mode is a prototype feature and does not yet detect all "
      "synchronizing operations");
  TORCH_CHECK(
      THPUtils_checkLong(arg),
      "invalid argument to set_sync_debug_mode, debug_mode type must long",
      PTA_ERROR(ErrCode::PARAM));
  int64_t debug_mode = THPUtils_unpackLong(arg);
  TORCH_CHECK(
      debug_mode >= 0 && debug_mode <= 2,
      "invalid value of debug_mode, expected one of 0,1,2",
      PTA_ERROR(ErrCode::VALUE));
  c10_npu::SyncDebugMode level;
  switch (debug_mode) {
    case 0:
      level = c10_npu::SyncDebugMode::L_DISABLED;
      break;
    case 1:
      level = c10_npu::SyncDebugMode::L_WARN;
      break;
    case 2:
      level = c10_npu::SyncDebugMode::L_ERROR;
      break;
    default:
      level = c10_npu::SyncDebugMode::L_DISABLED;
      break;
  }
  c10_npu::warning_state().set_sync_debug_mode(level);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_npu_get_sync_debug_mode(PyObject* self, PyObject* noargs) {
  HANDLE_TH_ERRORS
  auto debug_mode = c10_npu::warning_state().get_sync_debug_mode();
  switch (debug_mode) {
    case c10_npu::SyncDebugMode::L_DISABLED:
      return THPUtils_packInt32(0);
    case c10_npu::SyncDebugMode::L_WARN:
      return THPUtils_packInt32(1);
    case c10_npu::SyncDebugMode::L_ERROR:
      return THPUtils_packInt32(2);
    default:
      return THPUtils_packInt32(-1); // can't happen
  }
  END_HANDLE_TH_ERRORS
}

PyObject* THNPModule_tensor_construct_from_storage(
    PyObject* self,
    PyObject* args) {
  HANDLE_TH_ERRORS
  static torch::PythonArgParser parser(
      {
          "set_storage_with_format_(Storage source)",
      },
      /* traceable= */ false);

  torch::ParsedArgs<1> parsed_args;
  auto _r = parser.parse(args, nullptr, parsed_args);

  at::ScalarType storage_scalar_type;
  bool is_typed_storage = true;
  c10::Storage storage = _r.storage(0, storage_scalar_type, is_typed_storage);
  return THPVariable_Wrap(
      at_npu::native::set_tensor_with_storage_format(storage));

  END_HANDLE_TH_ERRORS
}

static struct PyMethodDef THNPModule_methods[] = {
    {"_npu_init", (PyCFunction)THNPModule_initExtension, METH_NOARGS, nullptr},
    {"_npu_set_run_yet_variable_to_false",
     (PyCFunction)THNPModule_set_run_yet_variable_to_false_wrap,
     METH_NOARGS,
     nullptr},
    {"_npu_getCurrentStream",
     (PyCFunction)THNPModule_getCurrentStream_wrap,
     METH_O,
     nullptr},
    {"_npu_getDefaultStream",
     (PyCFunction)THNPModule_getDefaultStream_wrap,
     METH_O,
     nullptr},
    {"_npu_setStream",
     (PyCFunction)THNPModule_setStream_wrap,
     METH_VARARGS | METH_KEYWORDS,
     nullptr},
    {"_npu_is_jit_compile_false",
     (PyCFunction)THNPModule_is_jit_compile_false_wrap,
     METH_NOARGS,
     nullptr},
    {"_npu_lock_mutex",
     (PyCFunction)THNPModule_npuLockMutex,
     METH_NOARGS,
     nullptr},
    {"_npu_unlock_mutex",
     (PyCFunction)THNPModule_npuUnlockMutex,
     METH_NOARGS,
     nullptr},
    {"_npu_setOption", (PyCFunction)THNPModule_setOption_wrap, METH_O, nullptr},
    {"_npu_get_soc_version",
     (PyCFunction)THNPModule_npu_get_soc_version,
     METH_NOARGS,
     nullptr},
    {"_enable_overflow_npu",
     (PyCFunction)THNPModule_enable_overflow_npu,
     METH_NOARGS,
     nullptr},
    {"_npu_is_support_inf_nan",
     (PyCFunction)THNPModule_npu_is_support_inf_nan,
     METH_NOARGS,
     nullptr},
    {"_npu_is_bf16_supported",
     (PyCFunction)THNPModule_npu_is_bf16_supported,
     METH_NOARGS,
     nullptr},
    {"_check_overflow_npu",
     (PyCFunction)THNPModule_check_overflow_npu,
     METH_NOARGS,
     nullptr},
    {"_clear_overflow_npu",
     (PyCFunction)THNPModule_clear_overflow_npu,
     METH_NOARGS,
     nullptr},
    {"_npu_getOption", (PyCFunction)THNPModule_getOption_wrap, METH_O, nullptr},
    {"_npu_set_sync_debug_mode",
     (PyCFunction)THNPModule_npu_set_sync_debug_mode,
     METH_O,
     nullptr},
    {"_npu_get_sync_debug_mode",
     (PyCFunction)THNPModule_npu_get_sync_debug_mode,
     METH_NOARGS,
     nullptr},
    {"_tensor_construct_from_storage",
     (PyCFunction)THNPModule_tensor_construct_from_storage,
     METH_VARARGS,
     nullptr},
    {nullptr}};

TORCH_BACKEND_API PyMethodDef* THNPModule_get_methods() {
  return THNPModule_methods;
}
