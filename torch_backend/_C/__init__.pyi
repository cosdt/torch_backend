# Defined in torch_backend/csrc/Module.cpp

from typing import Optional, List
import torch


# Defined in torch_backend/csrc/backend/Init.cpp
def _init() -> None: ...


# Defined in torch_backend/csrc/backend/Lock.cpp
def _lock_mutex() -> None: ...
def _unlock_mutex() -> None: ...


# Defined in torch_backend/csrc/backend/Device.cpp
def _synchronize() -> None: ...
def _setDevice(device: int) -> None: ...
def _getDevice() -> int: ...
def _getDeviceCount() -> int: ...
def _canDeviceAccessPeer(device_id: int, peer_device_id: int) -> bool: ...
def _npu_isHistoryEnabled() -> bool: ...
def _npu_getDeviceProperties(deviceid: int) -> _NPUDeviceProperties: ...


class _NPUDeviceProperties:
    name: str
    total_memory: int


# Defined in torch_backend/csrc/backend/Memory.cpp
def _setMemoryFraction(fraction: float, device: int) -> None: ...
def _emptyCache() -> None: ...
def _memoryStats(device: int) -> dict: ...
def _resetAccumulatedMemoryStats(device: int) -> None: ...
def _resetPeakMemoryStats(device: int) -> None: ...
def _memorySnapshot() -> dict: ...
def attach_out_of_memory_observer(device: int, alloc: int, device_allocated: int, device_free: int) -> None: ...
def _CachingAllocator_raw_alloc(size: int, stream_ptr: int) -> int: ...
def _CachingAllocator_raw_delete(mem_ptr: int) -> None: ...
def _getAllocatorBackend() -> str: ...


# Defined in torch_backend/csrc/backend/Stream.cpp
def _getCurrentStream(device_index: int) -> tuple: ...
def _getDefaultStream(device_index: int) -> tuple: ...
def _setStream(stream_id: Optional[int] = ..., device_index: Optional[int] = ..., device_type: Optional[int] = ...) -> None: ...


class _StreamBase:
    stream_id: int
    device_index: int
    device_type: int

    device: torch.device
    npu_stream: int
    priority: int

    def priority_range(self) -> None: ...
    def query(self) -> bool: ...
    def synchronize(self) -> None: ...

    @staticmethod
    def __new__(cls, priority: Optional[int] = ..., stream_id: Optional[int] = ..., device_index: Optional[int] = ...,
                device_type: Optional[int] = ..., stream_ptr: Optional[int] = ...) -> _StreamBase: ...


# Defined in torch_backend/csrc/core/python_tensor.cpp
def generate_tensor_types(p_list: List[object]) -> None: ...


# Defined in torch_backend/csrc/backend/Event.cpp
class _EventBase:
    device: torch.device
    npu_event: int

    def record(self, stream: _StreamBase) -> None: ...
    def wait(self, stream: _StreamBase) -> None: ...
    def query(self) -> bool: ...
    def elapsed_time(self, other: _EventBase) -> float: ...
    def synchronize(self) -> None: ...

    @staticmethod
    def __new__(cls, enable_timing: Optional[bool] = ..., blocking: Optional[bool] = ..., interprocess: Optional[bool] = ...) -> _EventBase: ...
