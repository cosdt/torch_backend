# Defined in torch_backend/csrc/Module.cpp

from typing import Optional, List


# Defined in torch_backend/csrc/backend/Init.cpp
def _init() -> None: ...


# Defined in torch_backend/csrc/backend/Lock.cpp
def _lock_mutex() -> None: ...
def _unlock_mutex() -> None: ...


# Defined in torch_backend/csrc/backend/Device.cpp
def _synchronize() -> None: ...
def _setDevice(device: int) -> None: ...
def _getDevice() -> int: ...
def _getDeviceCount() -> int: ...
def _canDeviceAccessPeer(device_id: int, peer_device_id: int) -> bool: ...


# Defined in torch_backend/csrc/backend/Memory.cpp
def _setMemoryFraction(fraction: float, device: int) -> None: ...
def _emptyCache() -> None: ...
def _memoryStats(device: int) -> dict: ...
def _resetAccumulatedMemoryStats(device: int) -> None: ...
def _resetPeakMemoryStats(device: int) -> None: ...
def _memorySnapshot() -> dict: ...
def attach_out_of_memory_observer(device: int, alloc: int, device_allocated: int, device_free: int) -> None: ...
def _CachingAllocator_raw_alloc(size: int, stream_ptr: int) -> int: ...
def _CachingAllocator_raw_delete(mem_ptr: int) -> None: ...
def _getAllocatorBackend() -> str: ...


# Defined in torch_backend/csrc/backend/Stream.cpp
def _getCurrentStream(device_index: int) -> tuple: ...
def _getDefaultStream(device_index: int) -> tuple: ...
def _setStream(stream_id: Optional[int] = ..., device_index: Optional[int] = ..., device_type: Optional[int] = ...) -> None: ...


# Defined in torch_backend/csrc/core/python_tensor.cpp
def generate_tensor_types(p_list: List[object]) -> None: ...
